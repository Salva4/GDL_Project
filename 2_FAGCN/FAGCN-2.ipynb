{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FAGCN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install spektral"
      ],
      "metadata": {
        "id": "kvh-dGR7Voqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import backend as K\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "F4oFHnuuTxzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Dropout, Input\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.random import set_seed\n",
        "\n",
        "from spektral.data.loaders import SingleLoader\n",
        "from spektral.datasets.citation import Citation\n",
        "from spektral.layers import GATConv\n",
        "from spektral.transforms import LayerPreprocess\n",
        "\n"
      ],
      "metadata": {
        "id": "Bqrt5FG2pg-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import scipy.sparse as sp\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score as acc\n",
        "\n",
        "\n",
        "def high_dim_gaussian(mu, sigma):\n",
        "    if mu.ndim > 1:\n",
        "        d = len(mu)\n",
        "        res = np.zeros(d)\n",
        "        for i in range(d):\n",
        "            res[i] = np.random.normal(mu[i], sigma[i])\n",
        "    else:\n",
        "        d = 1\n",
        "        res = np.zeros(d)\n",
        "        res = np.random.normal(mu, sigma)\n",
        "    return res\n",
        "\n",
        "\n",
        "def generate_uniform_theta(Y, c):\n",
        "    theta = np.zeros(len(Y), dtype='float')\n",
        "    for i in range(c):\n",
        "        idx = np.where(Y == i)\n",
        "        sample = np.random.uniform(low=0, high=1, size=len(idx[0]))\n",
        "        sample_sum = np.sum(sample)\n",
        "        for j in range(len(idx[0])):\n",
        "            theta[idx[0][j]] = sample[j] * len(idx[0]) / sample_sum\n",
        "    return theta\n",
        "\n",
        "\n",
        "def generate_theta_dirichlet(Y, c):\n",
        "    theta = np.zeros(len(Y), dtype='float')\n",
        "    for i in range(c):\n",
        "        idx = np.where(Y == i)\n",
        "        temp = np.random.uniform(low=0, high=1, size=len(idx[0]))\n",
        "        sample = np.random.dirichlet(temp, 1)\n",
        "        sample_sum = np.sum(sample)\n",
        "        for j in range(len(idx[0])):\n",
        "            theta[idx[0][j]] = sample[0][j] * len(idx[0]) / sample_sum\n",
        "    return theta\n",
        "    \n",
        "def SBM(sizes, probs, mus, sigmas, noise,\n",
        "        radius, feats_type='gaussian', selfloops=True):\n",
        "    # -----------------------------------------------\n",
        "    #     step1: get c,d,n\n",
        "    # -----------------------------------------------\n",
        "    c = len(sizes)\n",
        "    if mus.ndim > 1:\n",
        "        d = mus.shape[1]\n",
        "    else:\n",
        "        d = 1\n",
        "    n = sizes.sum()\n",
        "    all_node_ids = [ids for ids in range(0, n)]\n",
        "    # -----------------------------------------------\n",
        "    #     step2: generate Y with sizes\n",
        "    # -----------------------------------------------\n",
        "    Y = np.zeros(n, dtype='int')\n",
        "    for i in range(c):\n",
        "        class_i_ids = random.sample(all_node_ids, sizes[i])\n",
        "        Y[class_i_ids] = i\n",
        "        for item in class_i_ids:\n",
        "            all_node_ids.remove(item)\n",
        "    # -----------------------------------------------\n",
        "    #     step3: generate A with Y and probs\n",
        "    # -----------------------------------------------\n",
        "    if selfloops:\n",
        "        A = np.diag(np.ones(n, dtype='int'))\n",
        "    else:\n",
        "        A = np.zeros((n, n), dtype='int')\n",
        "    for i in range(n):\n",
        "        for j in range(i + 1, n):\n",
        "            prob_ = probs[Y[i]][Y[j]]\n",
        "            rand_ = random.random()\n",
        "            if rand_ <= prob_:\n",
        "                A[i][j] = 1\n",
        "                A[j][i] = 1\n",
        "    # -----------------------------------------------\n",
        "    #     step4: generate X with Y and mus, sigmas\n",
        "    # -----------------------------------------------\n",
        "    X = np.zeros((n, d), dtype='float')\n",
        "    for i in range(n):\n",
        "        mu = mus[Y[i]]\n",
        "        sigma = sigmas[Y[i]]\n",
        "        X[i] = high_dim_gaussian(mu, sigma)\n",
        "\n",
        "    return A, X, Y\n",
        "\n",
        "\n",
        "def generate(p, q, idx):\n",
        "    A, X, Y = \\\n",
        "        SBM(sizes=np.array([100, 100]),\n",
        "        probs=np.array([[p, q], [q, p]]),\n",
        "        mus=np.array([[-0.5]*20, [0.5]*20]),\n",
        "        sigmas=np.array([[2]*20, [2]*20]),\n",
        "        noise=[],\n",
        "        radius=[],\n",
        "        selfloops=False)\n",
        "        \n",
        "    return A, X, Y\n",
        "        \n",
        "        \n",
        "def calculate(A, X, Y):\n",
        "\n",
        "    A = sp.coo_matrix(A)\n",
        "    A = A + A.T.multiply(A.T > A) - A.multiply(A.T > A)\n",
        "    rowsum = np.array(A.sum(1)).clip(min=1)\n",
        "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
        "    A = A.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
        "\n",
        "    low = 0.5 * sp.eye(A.shape[0]) + A\n",
        "    high = 0.5 * sp.eye(A.shape[0]) - A\n",
        "    low = low.todense()\n",
        "    high = high.todense()\n",
        "\n",
        "    low_signal = np.dot(np.dot(low, low), X)\n",
        "    high_signal = np.dot(np.dot(high, high), X)\n",
        "\n",
        "    low_MLP = MLPClassifier(hidden_layer_sizes=(16), activation='relu', max_iter=2000)\n",
        "    low_MLP.fit(low_signal[:100, :], Y[:100])\n",
        "    low_pred = low_MLP.predict(low_signal[100:, :])\n",
        "\n",
        "    high_MLP = MLPClassifier(hidden_layer_sizes=(16), activation='relu', max_iter=2000)\n",
        "    high_MLP.fit(high_signal[:100, :], Y[:100])\n",
        "    high_pred = high_MLP.predict(high_signal[100:, :])\n",
        "\n",
        "    return acc(Y[100:], low_pred), acc(Y[100:], high_pred)\n",
        "\n",
        "\n",
        "low_record = []\n",
        "high_record = []\n",
        "\n",
        "\n",
        "for i in range(1, 11):\n",
        "    q = i * 0.01\n",
        "    p = 0.05\n",
        "    low_rec = []\n",
        "    high_rec = []\n",
        "    mlp_rec = []\n",
        "    print(i, p, q)\n",
        "\n",
        "    for j in range(10):\n",
        "        A, X, Y = generate(p, q, 0)\n",
        "        low, high, = calculate(A, X, Y)\n",
        "        low_rec.append(low)\n",
        "        high_rec.append(high)\n",
        "    low_record.append([np.max(low_rec), np.min(low_rec), np.mean(low_rec)])\n",
        "    high_record.append([np.max(high_rec), np.min(high_rec), np.mean(high_rec)])"
      ],
      "metadata": {
        "id": "ROYZUndy2FXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A,X,Y=generate(p, q,0)\n",
        "A1,X1,Y1=generate(p, q,0)"
      ],
      "metadata": {
        "id": "YkE9Sdy02Ixl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spektral.transforms.adj_to_sp_tensor import AdjToSpTensor"
      ],
      "metadata": {
        "id": "KObczAqe2TNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L = []\n",
        "for i in range(1, 11):\n",
        "    q = i * 0.01\n",
        "    p = 0.05\n",
        "    A,X,Y = generate(p, q,0)\n",
        "    L.append([p,q,A,X,Y])"
      ],
      "metadata": {
        "id": "glDZnxE-38Hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
      ],
      "metadata": {
        "id": "FPcCcHuSPUX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spektral.data.graph as gg\n",
        "from scipy import sparse"
      ],
      "metadata": {
        "id": "kU_fFwLW9W7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import itertools\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class DictionaryLookupDataset(object):\n",
        "    def __init__(self, L):\n",
        "        super().__init__()\n",
        "        self.L = L\n",
        "        \n",
        "    def generate_data(self):\n",
        "      indices = np.argwhere(self.L[2]==1)\n",
        "      edge_index = sparse.csr_matrix((np.ones(len(indices.T[0])),(indices.T[0],indices.T[1])),shape=(200,200))\n",
        "\n",
        "      nodes = self.L[3]\n",
        "            \n",
        "      C = np.zeros((len(self.L[-1]),2))\n",
        "      for i,j in enumerate(self.L[-1]):\n",
        "        if j==0:\n",
        "          C[i] = np.array([1,0])\n",
        "        else:\n",
        "          C[i] = np.array([0,1]) \n",
        "      LL=[]\n",
        "      LL.append(gg.Graph(x=nodes, a=edge_index, y=C))\n",
        "\n",
        "      return LL"
      ],
      "metadata": {
        "id": "z1sv3JvAxYtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import itertools\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class DictionaryLookupDataset(object):\n",
        "    def __init__(self, L):\n",
        "        super().__init__()\n",
        "        self.L = L\n",
        "        \n",
        "    def generate_data(self):\n",
        "      indices = np.argwhere(self.L[2]==1)\n",
        "      edge_index = sparse.csr_matrix((np.ones(len(indices.T[0])),(indices.T[0],indices.T[1])),shape=(200,200))\n",
        "\n",
        "      nodes = self.L[3]\n",
        "            \n",
        "      C = np.zeros((len(self.L[-1]),2))\n",
        "      for i,j in enumerate(self.L[-1]):\n",
        "        if j==0:\n",
        "          C[i] = np.array([1,0])\n",
        "        else:\n",
        "          C[i] = np.array([0,1]) \n",
        "      LL_t=[]\n",
        "      LL_v=[]\n",
        "      pool0 = np.argwhere(self.L[-1]==0)\n",
        "      pool1 = np.argwhere(self.L[-1]==1)\n",
        "      \n",
        "      np.random.shuffle(pool0)\n",
        "      np.random.shuffle(pool1)\n",
        "      tr = np.concatenate([pool0[0:50],pool1[0:50]])\n",
        "      va = np.concatenate([pool0[50:],pool1[50:]])\n",
        "      np.random.shuffle(tr)\n",
        "      np.random.shuffle(va)\n",
        "      \n",
        "      C_tr = C[tr.flatten()] \n",
        "      C_va = C[va.flatten()] \n",
        "      LL_t.append(gg.Graph(x=nodes, a=edge_index, y=C_tr))\n",
        "      LL_v.append(gg.Graph(x=nodes, a=edge_index, y=C_va))\n",
        "\n",
        "      return LL_t,LL_v,tr.flatten(),va.flatten()\n"
      ],
      "metadata": {
        "id": "xgI1JfGBYGK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id = -1\n",
        "dictionary = DictionaryLookupDataset(L[id])"
      ],
      "metadata": {
        "id": "jsKteCF2vClK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LL=dictionary.generate_data()"
      ],
      "metadata": {
        "id": "y9-OM-D8_bDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spektral.data import Dataset"
      ],
      "metadata": {
        "id": "JsSPNVIXTwiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A dataset of five random graphs.\n",
        "    \"\"\"\n",
        "    def __init__(self, list_g, **kwargs):\n",
        "        self.list_g = list_g\n",
        "\n",
        "        super().__init__(**kwargs)\n",
        "    \n",
        "    def read(self):\n",
        "      return self.list_g"
      ],
      "metadata": {
        "id": "Ct6T29_LZ9pK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = MyDataset(LL[0])\n",
        "val = MyDataset(LL[1])"
      ],
      "metadata": {
        "id": "EhvM3Hi-Bcsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spektral.utils.convolution import degree_power,add_self_loops, normalized_adjacency\n",
        "from spektral.transforms import gcn_filter"
      ],
      "metadata": {
        "id": "yGDM64antpfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ad = np.copy(L[id][2])\n",
        "final = normalized_adjacency(ad)\n",
        "final=final.astype('float32')\n",
        "final = tf.convert_to_tensor(final)"
      ],
      "metadata": {
        "id": "E7_eWWyV9X-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMPbOOSjCelh",
        "outputId": "7f0a4135-5ad3-4590-9759-8e712d5201e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(200, 200), dtype=float32, numpy=\n",
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 291
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#VANILLA ATTENTION NETWORK  \n",
        "from tensorflow.keras import constraints, initializers, regularizers\n",
        "\n",
        "from spektral.layers import ops\n",
        "from spektral.layers.convolutional.conv import Conv\n",
        "from spektral.layers.ops import modes\n",
        "\n",
        "\n",
        "class FAGCN(Conv):\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels,\n",
        "        eps=0.3,\n",
        "        L=5,\n",
        "        out=2,\n",
        "        deg=None,\n",
        "        dropout_rate=0.5,\n",
        "        add_self_loops=False,\n",
        "        activation=None,\n",
        "        use_bias=True,\n",
        "        kernel_initializer=\"glorot_uniform\",\n",
        "        bias_initializer=\"zeros\",\n",
        "        kernel_regularizer=None,\n",
        "        bias_regularizer=None,\n",
        "        activity_regularizer=None,\n",
        "        kernel_constraint=None,\n",
        "        bias_constraint=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(\n",
        "            activation=activation,\n",
        "            use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer,\n",
        "            bias_initializer=bias_initializer,\n",
        "            kernel_regularizer=kernel_regularizer,\n",
        "            bias_regularizer=bias_regularizer,\n",
        "            activity_regularizer=activity_regularizer,\n",
        "            kernel_constraint=kernel_constraint,\n",
        "            bias_constraint=bias_constraint,\n",
        "            **kwargs\n",
        "        )\n",
        "        self.channels = channels\n",
        "        self.eps = eps\n",
        "        self.L = L\n",
        "        self.deg = deg\n",
        "        self.out = out\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.add_self_loops = add_self_loops\n",
        "        \n",
        "      \n",
        "        self.output_dim = out\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim = input_shape[0][-1]\n",
        "        self.kernel1 = self.add_weight(\n",
        "            name=\"kernel1\",\n",
        "            shape=[input_dim, self.channels],\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "        )\n",
        "        self.kernel2 = self.add_weight(\n",
        "            name=\"kernel2\",\n",
        "            shape=[self.channels, self.channels],\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "        )\n",
        "        self.kernel3 = self.add_weight(\n",
        "            name=\"kernel3\",\n",
        "            shape=[self.channels,self.out ],\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "        )\n",
        "\n",
        "\n",
        "        \n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(\n",
        "                shape=[self.output_dim],\n",
        "                initializer=self.bias_initializer,\n",
        "                regularizer=self.bias_regularizer,\n",
        "                constraint=self.bias_constraint,\n",
        "                name=\"bias\",\n",
        "            )\n",
        "\n",
        "        self.dropout = Dropout(self.dropout_rate, dtype=self.dtype)\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        x, a = inputs\n",
        "  \n",
        "       \n",
        "        output = self._call_single(x, a)\n",
        "        \n",
        "        if self.use_bias:\n",
        "            output += self.bias\n",
        "        if mask is not None:\n",
        "            output *= mask[0]\n",
        "        output = self.activation(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _call_single(self, x, a):\n",
        "        \n",
        "        \n",
        "        indices = a.indices\n",
        "     \n",
        "\n",
        "        N = tf.shape(x, out_type=indices.dtype)[-2]\n",
        "        \n",
        "        if self.add_self_loops:\n",
        "            indices = ops.add_self_loops_indices(indices, N)\n",
        "       \n",
        "        targets, sources = indices[:, 1], indices[:, 0]\n",
        "        \n",
        "        x = K.dot(x, self.kernel1)\n",
        "        x = tf.reshape(x, (-1, self.channels))\n",
        "        raw = tf.keras.layers.ReLU()(x)\n",
        "        raw = self.dropout(raw)\n",
        "        h = raw\n",
        "        for layers in range(0,self.L):\n",
        "          gh = K.dot(h, self.kernel2)\n",
        "          theta1 = tf.gather(gh, targets)\n",
        "          theta2 = tf.gather(gh, sources)\n",
        "          alpha = tf.math.tanh(theta1+theta2)\n",
        "          #alpha = 1 * tf.math.abs(alpha) <---- low filter: uncomment\n",
        "          #alpha = -1 * tf.math.abs(alpha) <---- high filter: uncomment\n",
        "\n",
        "       \n",
        "          indexes = tf.concat([tf.expand_dims(targets, axis=1),tf.expand_dims(sources, axis=1)],1)\n",
        "          div = tf.gather_nd(indices=indexes,params=self.deg)\n",
        "          div = div[...,None]\n",
        "          alpha = alpha*div\n",
        "          alpha = self.dropout(alpha)\n",
        "          sums = tf.math.multiply(alpha, tf.gather(h, sources)) \n",
        "          sums = tf.math.unsorted_segment_sum(sums, targets,N) #len(tf.gather(raw, targets)))\n",
        "          h = self.eps*raw +sums\n",
        "\n",
        "        output = K.dot(h, self.kernel3)\n",
        "        \n",
        "        return output\n",
        "\n",
        " \n",
        "\n",
        "    @property\n",
        "    def config(self):\n",
        "        return {\n",
        "            \"channels\": self.channels,\n",
        "            \"attn_heads\": self.attn_heads,\n",
        "            \"concat_heads\": self.concat_heads,\n",
        "            \"dropout_rate\": self.dropout_rate,\n",
        "            \"return_attn_coef\": self.return_attn_coef,\n",
        "            \"attn_kernel_initializer\": initializers.serialize(\n",
        "                self.attn_kernel_initializer\n",
        "            ),\n",
        "            \"attn_kernel_regularizer\": regularizers.serialize(\n",
        "                self.attn_kernel_regularizer\n",
        "            ),\n",
        "            \"attn_kernel_constraint\": constraints.serialize(\n",
        "                self.attn_kernel_constraint\n",
        "            ),\n",
        "        }"
      ],
      "metadata": {
        "id": "9tdQ1Z1O9H3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = 200  # Number of nodes in the graph\n",
        "F = 20  # Original size of node features\n",
        "n_out = 2  # Number of classes\n"
      ],
      "metadata": {
        "id": "jNKkduhAgBKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "channels = 20  # Number of channels in each head of the first GAT layer\n",
        "dropout = 0.0  # Dropout rate for the features and adjacency matrix\n",
        "learning_rate = 0.01  # Learning rate\n",
        "epochs = 20000  # Number of training epochs\n",
        "patience = 100  # Patience for early stopping\n",
        "\n",
        "# Model definition\n",
        "x_in = Input(shape=(F))\n",
        "a_in = Input((N), sparse=True)"
      ],
      "metadata": {
        "id": "hZKu_6XG9o1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#do_1 = Dropout(dropout)(attr)\n",
        "gc_1 = FAGCN(\n",
        "    channels,\n",
        "    eps=0.2,\n",
        "    L=1,\n",
        "    deg = final,\n",
        "    dropout_rate=dropout,\n",
        "    activation=\"softmax\",\n",
        ")([x_in, a_in])"
      ],
      "metadata": {
        "id": "sXmVzQniBHGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(inputs=[x_in, a_in], outputs=gc_1)\n",
        "optimizer = Adam(learning_rate=learning_rate)\n"
      ],
      "metadata": {
        "id": "-6kWgP3nkON6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "hkKoD_ygBtSt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5aa2e2f-3170-45c6-cb05-2bb088ef9442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_16\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_29 (InputLayer)          [(None, 20)]         0           []                               \n",
            "                                                                                                  \n",
            " input_30 (InputLayer)          [(None, 200)]        0           []                               \n",
            "                                                                                                  \n",
            " fagcn_16 (FAGCN)               (None, 2)            842         ['input_29[0][0]',               \n",
            "                                                                  'input_30[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 842\n",
            "Trainable params: 842\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader_tr = SingleLoader(train)\n",
        "loader_va = SingleLoader(val)"
      ],
      "metadata": {
        "id": "bdyIItCmAtn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "# Instantiate an optimizer.\n",
        "\n",
        "# Instantiate a loss function.\n",
        "loss_fn =  tf.keras.losses.BinaryCrossentropy(from_logits=False)"
      ],
      "metadata": {
        "id": "nM_RYIXYCtrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAIN"
      ],
      "metadata": {
        "id": "3w2Th211CiN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 300\n",
        "acc=0\n",
        "running_loss = 0\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, g in enumerate(train):\n",
        "        inputs, target = loader_tr.__next__()\n",
        "        \n",
        "        # Open a GradientTape to record the operations run\n",
        "        # during the forward pass, which enables auto-differentiation.\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            # Run the forward pass of the layer.\n",
        "            # The operations that the layer applies\n",
        "            # to its inputs are going to be recorded\n",
        "            # on the GradientTape.\n",
        "\n",
        "            logits = model(inputs, training=True)  # Logits for this minibatch\n",
        "            logits = tf.gather(logits,LL[2])\n",
        "            # Compute the loss value for this minibatch.\n",
        "            loss_value = loss_fn(target, logits)\n",
        "            acc = sum(tf.argmax(logits,1).numpy()==tf.argmax(target,1).numpy())/len(target)\n",
        "\n",
        "        # Use the gradient tape to automatically retrieve\n",
        "        # the gradients of the trainable variables with respect to the loss.\n",
        "        \n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        inputs, target = loader_va.__next__()\n",
        "        val_logits = model(inputs, training=False)\n",
        "        val_logits = tf.gather(val_logits,LL[3])\n",
        "           \n",
        "        loss_value_v = loss_fn(target, val_logits)\n",
        "        acc_v = sum(tf.argmax(val_logits,1).numpy()==tf.argmax(target,1).numpy())/len(target)\n",
        "\n",
        "        print(\n",
        "            \"Training loss (for one batch) at step %d: %.4f\"\n",
        "            % (step, float(loss_value))\n",
        "        )\n",
        "        \n",
        "        print(\n",
        "            \"train-Accuracy (for one batch) at step %d: %.4f\"\n",
        "            % (step, float(acc))\n",
        "        )\n",
        "        print(\n",
        "            \"val loss (for one batch) at step %d: %.4f\"\n",
        "            % (step, float(loss_value_v))\n",
        "        )        \n",
        "        print(\n",
        "            \"val-Accuracy (for one batch) at step %d: %.4f\"\n",
        "            % (step, float(acc_v))\n",
        "        )\n",
        " \n",
        "        print(\"Seen so far: %s samples\" % ((step + 1)))\n",
        "        "
      ],
      "metadata": {
        "id": "WCUo5-sLSpVx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}